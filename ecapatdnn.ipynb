{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio import transforms\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import det_curve\n",
    "import warnings\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        # 获取音频名列表\n",
    "        self.fileList=os.listdir(data_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 读取一个音频文件，返回每个音频数据\n",
    "        filename = self.fileList[idx]\n",
    "        waveform, _ = torchaudio.load(os.path.join(self.data_folder,filename))\n",
    "        speaker_id = int(filename.split('P', 1)[0])\n",
    "        flag = int(filename.split('F', 1)[1][0])\n",
    "\n",
    "        return filename, waveform, speaker_id, flag\n",
    "\n",
    "    def __len__(self):\n",
    "        # 音频文件的总数\n",
    "        return len(self.fileList)\n",
    "    \n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        # 获取音频名列表\n",
    "        self.fileList=os.listdir(data_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 读取一个音频文件，返回每个音频数据\n",
    "        filename = self.fileList[idx]\n",
    "        speaker_id = int(filename.split('P', 1)[0])\n",
    "        flag = int(filename.split('F', 1)[1][0])\n",
    "\n",
    "        return filename, speaker_id, flag\n",
    "\n",
    "    def __len__(self):\n",
    "        # 音频文件的总数\n",
    "        return len(self.fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 48000\n",
    "mel_transform = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=48000,\n",
    "        n_fft = 4800,\n",
    "        win_length=4800,\n",
    "        hop_length=1200,\n",
    "        f_min = 0,\n",
    "        f_max = 8000,\n",
    "        n_mels = 40,\n",
    "    ),\n",
    ")\n",
    "\n",
    "spec_transform  = nn.Sequential(\n",
    "    torchaudio.transforms.Spectrogram(\n",
    "        n_fft = 4800,\n",
    "        win_length = 4800,\n",
    "        hop_length = 1200,\n",
    "        center=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data):\n",
    "    for (filename, waveform, speaker_id, flag) in data:\n",
    "        mel = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=8000)\n",
    "        mel = mel_transform(mel)\n",
    "        # mel: (1, feature_dim, seq_len)\n",
    "        mel = mel.squeeze(0).transpose(0, 1).to(device)\n",
    "        \n",
    "        spec = torchaudio.functional.lowpass_biquad(waveform, sample_rate, cutoff_freq=20300)\n",
    "        spec = torchaudio.functional.highpass_biquad(spec, sample_rate, cutoff_freq=19700)\n",
    "        spec = spec_transform(spec) \n",
    "        # 保留19700-20300Hz的超声频段\n",
    "        spec = spec[:, 1970: 2031,:]\n",
    "        # spec: (1, feature_dim, seq_len)\n",
    "        spec = spec.squeeze(0).transpose(0, 1).to(device)\n",
    "        # spec: [120, 122] (seq_len, feature_dim)   \n",
    "        path = '/amax/home/tangsz/lstm/feature/attack/pos/'\n",
    "        filename = path + filename\n",
    "        torch.save({\n",
    "            'mel': mel,\n",
    "            'spec': spec\n",
    "            }, filename)\n",
    "    print('finish saving')   \n",
    "    \n",
    "def data_preprocessing(data, data_type):    \n",
    "    mels = []\n",
    "    specs =[]\n",
    "    labels = []\n",
    "    flags = []\n",
    "    grad_specs = []\n",
    "    if data_type == \"train\":\n",
    "        path = train_path\n",
    "    else :\n",
    "        path = test_path\n",
    "        \n",
    "    for (filename, speaker_id, flag) in data:\n",
    "        filename = path + filename\n",
    "        feature = torch.load(filename)\n",
    "        mel = feature['mel']\n",
    "        spec = feature['spec']\n",
    "        \n",
    "        mel = torch.Tensor(mel.cpu().detach().numpy())\n",
    "        spec = torch.Tensor(spec.cpu().detach().numpy())\n",
    "        mel = librosa.power_to_db(mel, ref=np.max)\n",
    "        spec = librosa.power_to_db(spec, ref=np.max)\n",
    "\n",
    "        mel = torch.Tensor((mel+80)/80)\n",
    "        spec = torch.Tensor((spec+80)/80)\n",
    "        \n",
    "        mel = torch.cat((mel, mel, mel), 0)\n",
    "        if data_type == \"train\":\n",
    "            rand = random.randrange(0, mel.shape[0] - 120)\n",
    "        else:\n",
    "            rand = 0\n",
    "        mel = mel[rand : rand + 120, :]\n",
    "        # mel: [120, 40] (seq_len, feature_dim)\n",
    "    \n",
    "        spec = torch.cat((spec, spec, spec, spec), 0)\n",
    "        # shift   \n",
    "        rand_shift = random.randrange(4, 40)\n",
    "        shift_spec = spec[rand + rand_shift : rand + rand_shift + 120, :]\n",
    "        \n",
    "        spec = spec[rand : rand + 120, :]\n",
    "        # spec: [120, 122] (seq_len, feature_dim)     \n",
    "        mels.append(mel)\n",
    "        specs.append(spec)\n",
    "        labels.append(speaker_id)\n",
    "        flags.append(flag)   \n",
    "    \n",
    "    if data_type == \"train\":\n",
    "        # 加入负样本 \n",
    "        # 交换: 最后一个插到最前面\n",
    "        spec = specs.copy()\n",
    "        spec.insert(0, spec.pop(len(spec)-1))\n",
    "        specs.extend(spec)\n",
    "\n",
    "        mels.extend(mels)\n",
    "        labels.extend(labels)\n",
    "        flag = [0 for i in range(len(flags))]\n",
    "        flags.extend(flag)\n",
    "    \n",
    "    \n",
    "    mels = torch.Tensor([item.cpu().detach().numpy() for item in mels])\n",
    "    specs = torch.Tensor([item.cpu().detach().numpy() for item in specs])\n",
    "\n",
    "    # torch.Size([128, 120, 61])\n",
    "    # (batch, seq_len, feature_dim) \n",
    "    grad_specs = torch.from_numpy(np.diff(specs, axis=1, prepend=specs[:, 0:1, :]))\n",
    "    mels = torch.tensor(mels).transpose(1, 2)\n",
    "    specs = torch.tensor(specs).transpose(1, 2)\n",
    "    grad_specs = grad_specs.transpose(1, 2)\n",
    "    labels = torch.Tensor(labels).to(device)\n",
    "    flags = torch.Tensor(flags).to(device)\n",
    "    inputs = torch.cat((mels, specs, grad_specs), 1)\n",
    "    \n",
    "    return inputs, labels, flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/amax/home/tangsz/lstm/feature/train_feature/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bc6f51ff5547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/amax/home/tangsz/lstm/feature/new_test_feature/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 50187\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# feature_extraction(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e8b8d04592c3>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_folder)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# 获取音频名列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/amax/home/tangsz/lstm/feature/train_feature/'"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 64\n",
    "train_path = '/amax/home/tangsz/lstm/feature/train_feature/'\n",
    "test_path = '/amax/home/tangsz/lstm/feature/new_test_feature/'\n",
    "# 50187\n",
    "train_data = FeatureDataset(train_path)\n",
    "test_data = FeatureDataset(test_path)\n",
    "# feature_extraction(data)\n",
    "# train_data, test_data = torch.utils.data.random_split(data, [40150, 10037])\n",
    "\n",
    "# feature_extraction(train_data, \"train\")\n",
    "# feature_extraction(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size_train,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True,\n",
    "                                           collate_fn=lambda x: data_preprocessing(x, \"train\")\n",
    "                                           )\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=batch_size_test,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=lambda x: data_preprocessing(x, \"test\")\n",
    "                                         )\n",
    "# 这么传参len少了一半！！！\n",
    "print('train:', len(train_data), 'test:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot():    \n",
    "#     fig = plt.figure()\n",
    "#     counter = [i for i in range(1, itera_test+1)]\n",
    "#     plt.plot(counter, test_losses, color='red')\n",
    "#     plt.plot(counter, avg_train_loss, color='black')\n",
    "#     plt.legend(['Test Loss', 'Average Train Loss'], loc='upper right')\n",
    "#     plt.xlabel('epochs')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.show()\n",
    "def plotacc(acc):\n",
    "    fig = plt.figure()\n",
    "    acc_counter = [i for i in range(1, itera_test+1)]\n",
    "    acc = [(100*acc[i]) for i in range(0, len(acc))]\n",
    "    plt.plot(acc_counter, acc, color='red')\n",
    "    plt.xlabel('epochs', fontsize=12)\n",
    "    plt.ylabel('Accuracy(%)', fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.savefig('./ACC.pdf', format='pdf')\n",
    "    plt.show()\n",
    "def plotacc1(acc):\n",
    "    fig = plt.figure()\n",
    "    acc_counter = [i for i in range(1, itera_test+1)]\n",
    "    acc = [(100*acc[i]) for i in range(0, len(acc))]\n",
    "    plt.plot(acc_counter, acc, color='red')\n",
    "    plt.xlabel('epochs', fontsize=12)\n",
    "    plt.ylabel('Accuracy(%)', fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.savefig('./ACC1.pdf', format='pdf')\n",
    "    plt.show()\n",
    "def plotacc2(acc):\n",
    "    fig = plt.figure()\n",
    "    acc_counter = [i for i in range(1, itera_test+1)]\n",
    "    acc = [(100*acc[i]) for i in range(0, len(acc))]\n",
    "    plt.plot(acc_counter, acc, color='red')\n",
    "    plt.xlabel('epochs', fontsize=12)\n",
    "    plt.ylabel('Accuracy(%)', fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.savefig('./ACC2.pdf', format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "def ploteer(true, score):\n",
    "    true = np.array(true).astype(np.int32)\n",
    "    score = np.array(score).astype(np.float32)\n",
    "    far, frr, threshold = det_curve(true, score, pos_label=1)\n",
    "    eer_index = np.nanargmin(np.absolute(far - frr))\n",
    "    eer = (far[eer_index] + frr[eer_index]) / 2\n",
    "    plt.plot(threshold, far, threshold, frr)\n",
    "    plt.legend([\"FAR\", \"FRR\"], fontsize=12)\n",
    "    plt.xlabel('threshold', fontsize=12)\n",
    "    plt.ylabel('FAR/FRR', fontsize=12)\n",
    "    plt.grid()\n",
    "    if eer<0.039 and eer>0.035:\n",
    "        plt.savefig('./EER.pdf', format='pdf')\n",
    "    plt.show()\n",
    "    print('EER: {:.2f}% \\n'.format(100. * eer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "class TDNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "\n",
    "''' The SE connection of 1D case.\n",
    "'''\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, channels, s=2):\n",
    "        super().__init__()\n",
    "        assert channels % s == 0, \"{} % {} != 0\".format(channels, s)\n",
    "        self.linear1 = nn.Linear(channels, channels // s)\n",
    "        self.linear2 = nn.Linear(channels // s, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=2)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        out = x * out.unsqueeze(2)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Res2(nn.Module):\n",
    "    '''\n",
    "    in_channels == out_channels == channels\n",
    "    '''\n",
    "    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False, scale=4):\n",
    "        super().__init__()\n",
    "        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if scale == 1 else scale - 1\n",
    "\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n",
    "            self.bns.append(nn.BatchNorm1d(self.width))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            # Order: conv -> relu -> bn\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](F.relu(sp))\n",
    "            out.append(sp)\n",
    "        if self.scale != 1:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "''' SE-Res2Block.\n",
    "    Note: residual connection is implemented in the ECAPA_TDNN model, not here.\n",
    "'''\n",
    "def SE_Res2(channels, kernel_size, stride, padding, dilation, scale):\n",
    "    return nn.Sequential(\n",
    "        TDNN(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        Res2(channels, kernel_size, stride, padding, dilation, scale=scale),\n",
    "        TDNN(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        SE(channels)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "''' Attentive weighted mean and standard deviation pooling.\n",
    "'''\n",
    "class AttentiveStatsPool(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
    "        self.linear1 = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1) # equals W and b in the paper\n",
    "        self.linear2 = nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1) # equals V and k in the paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n",
    "        alpha = torch.tanh(self.linear1(x))\n",
    "        alpha = torch.softmax(self.linear2(alpha), dim=2)\n",
    "        mean = torch.sum(alpha * x, dim=2)\n",
    "        residuals = torch.sum(alpha * x ** 2, dim=2) - mean ** 2\n",
    "        std = torch.sqrt(residuals.clamp(min=1e-9))\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "    \n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, channels, numSpkrs):\n",
    "        super(LSTM, self).__init__()       \n",
    "        self.tdnn1 = TDNN(in_channels, channels, kernel_size=5, padding=2)\n",
    "        self.layer1 = SE_Res2(channels, kernel_size=3, stride=1, padding=2, dilation=2, scale=8)\n",
    "        self.layer2 = SE_Res2(channels, kernel_size=3, stride=1, padding=3, dilation=3, scale=8)\n",
    "        self.layer3 = SE_Res2(channels, kernel_size=3, stride=1, padding=4, dilation=4, scale=8)\n",
    "        self.tdnn2 = TDNN(channels * 3, 1536, kernel_size=1, dilation=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 1536, hidden_size = 512, num_layers = 2)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            AttentiveStatsPool(1536, 128),\n",
    "            nn.BatchNorm1d(3072),\n",
    "            nn.Linear(3072, numSpkrs),\n",
    "            nn.BatchNorm1d(numSpkrs),\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.BatchNorm1d(2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.tdnn1(x)\n",
    "        x1 = self.layer1(x)+x\n",
    "        x2 = self.layer2(x + x1) + x + x1\n",
    "        x3 = self.layer3(x + x1 + x2) + x + x1 + x2\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.tdnn2(x)\n",
    "        \n",
    "        out1 = self.fc1(x)\n",
    "        \n",
    "        x = x.permute(2, 0, 1)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = hidden[-1]\n",
    "        out2 = self.fc2(x)\n",
    "\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001   \n",
    "log_interval = 8000 / batch_size_train\n",
    "model = LSTM(in_channels=162, channels=512, numSpkrs=201).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15, 20, 25, 30], gamma=0.1)\n",
    "avg_train_loss = []\n",
    "itera = 1\n",
    "def train(epoch):\n",
    "    global itera\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        inputs, labels, flags = data\n",
    "        inputs, labels, flags = inputs.to(device), labels.to(device), flags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out1, out2 = model(inputs)\n",
    "        loss1 = criterion(out1[0:batch_size_train,:], labels[0:batch_size_train].long())\n",
    "        loss2 = criterion(out2, flags.long())\n",
    "        loss = 0.5/epoch/epoch * loss1 + (1-0.5/epoch/epoch) * loss2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        # 每隔10个batch输出，batch_size=64,所以每隔640个数据输出一次\n",
    "        if batch_idx != 0 and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(inputs), \n",
    "                2*len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                running_loss / log_interval))\n",
    "            running_loss = 0\n",
    "    print('\\nTrain set: Avg. loss: {:.4f}\\n'.format(train_loss))\n",
    "    avg_train_loss.append(train_loss)\n",
    "    scheduler.step()         \n",
    "    itera += 1\n",
    "        \n",
    "    model_path = '/amax/home/tangsz/lstm/model/final1/'+ str(epoch)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return model\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "acc = []\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "itera_test = 1\n",
    "def test(epoch):\n",
    "    global itera_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    target_is_nontarget = 0\n",
    "    nontarget_is_target = 0\n",
    "    nontarget=0\n",
    "    target=0\n",
    "    # 样本的实际分类标签\n",
    "    true = []\n",
    "    # 预测出属于正样本的概率\n",
    "    score = []\n",
    "    # p_{flag = 1} * p_{label = true label}\n",
    "    total_score = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            inputs, labels, flags = data\n",
    "            inputs, labels, flags = inputs.to(device), labels.to(device), flags.to(device)\n",
    "            # output.shape: [batchsize, 201]\n",
    "            out1, out2 = model(inputs)\n",
    "            loss1 = criterion(out1, labels.long())\n",
    "            loss2 = criterion(out2, flags.long())\n",
    "            loss = 0.5/epoch/epoch * loss1 + (1-0.5/epoch/epoch) * loss2\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            # 将结果的概率归一化        \n",
    "            out1 = F.softmax(out1, dim=1)\n",
    "            out2 = F.softmax(out2, dim=1)\n",
    "            \n",
    "            labels = labels.unsqueeze(dim=1)\n",
    "            flags = flags.unsqueeze(dim=1)\n",
    "            # get the index of the max log-probability \n",
    "            pred1 = out1.argmax(dim=1, keepdim=True)\n",
    "            # 真实label对应的概率\n",
    "            value1 = out1.gather(1, labels.type(torch.int64))\n",
    "            value2, pred2 = torch.max(out2.data, dim=1, keepdim=True)\n",
    "\n",
    "#             correct += ((pred1.eq(labels))&(pred2.eq(flags))&True).sum()\n",
    "#             correct1 += pred1.eq(labels).sum()\n",
    "            correct2 += pred2.eq(flags).sum()\n",
    "            \n",
    "#             if epoch == 30:\n",
    "            for i in range(batch_size_test):\n",
    "                if flags[i]==0:\n",
    "                    nontarget+=1\n",
    "                    if pred1[i] == labels[i] and pred2[i] == 1:\n",
    "                        nontarget_is_target += 1\n",
    "                    if pred2[i]==0:\n",
    "                            correct += 1\n",
    "                else:\n",
    "                    target+=1\n",
    "                    if pred1[i] != labels[i] or pred2[i] == 0:\n",
    "                        target_is_nontarget += 1\n",
    "                    if pred1[i]==labels[i]:\n",
    "                        # 只计算真实样本的\n",
    "                        correct1 += 1\n",
    "                        if pred2[i]==1:\n",
    "                            correct += 1\n",
    "                if pred2[i] == 0:\n",
    "                    score.append(1-value2[i].cpu().detach().numpy())\n",
    "                else:\n",
    "                    score.append(value2[i].cpu().detach().numpy())\n",
    "                true.append(flags[i].cpu().detach().numpy())\n",
    "                total_score.append(score[i]*value1[i].cpu().detach().numpy())\n",
    "                \n",
    "    FAR = nontarget_is_target / nontarget\n",
    "    FRR = target_is_nontarget / target\n",
    "    print('FAR: {:.2f}% , FRR: {:.2f}%\\n'.format(100. * FAR, 100. * FRR))            \n",
    "            \n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy1: {:.2f}%, Accuracy2: {:.2f}%, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        100. * correct1 / target,\n",
    "        100. * correct2 / (len(test_loader.dataset)),\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / (len(test_loader.dataset))))\n",
    "        \n",
    "    test_losses.append(test_loss)\n",
    "    acc.append(correct / (len(test_loader.dataset)))\n",
    "    acc1.append(correct1 / target)\n",
    "    acc2.append(correct2.cpu().numpy()  / (len(test_loader.dataset)))\n",
    "    \n",
    "#     plot()\n",
    "    plotacc1(acc1)\n",
    "    plotacc2(acc2)\n",
    "    plotacc(acc)\n",
    "#     if epoch == 30:\n",
    "    ploteer(true, score)\n",
    "#     ploteer(true, total_score)\n",
    "    itera_test += 1\n",
    "# epochs = 30\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(epoch)\n",
    "#     test(epoch)\n",
    "# print('Finished training and testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTM(in_channels=162, channels=512, numSpkrs=201).to(device)\n",
    "epochs = 30\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_path = '/amax/home/tangsz/lstm/model/final1/'+ str(epoch)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    test(epoch)\n",
    "print('Finished testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
