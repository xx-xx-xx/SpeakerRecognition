{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio import transforms\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# sample_rate=16000\n",
    "# n_mfcc = 40\n",
    "# n_fft = 512\n",
    "# n_mels = 256\n",
    "# hop_length = 10\n",
    "\n",
    "# mfcc_transform = nn.Sequential(\n",
    "#     torchaudio.transforms.MFCC(\n",
    "#         sample_rate=sample_rate,\n",
    "#         n_mfcc=n_mfcc,\n",
    "#         melkwargs={\n",
    "#           'n_fft': n_fft,\n",
    "#           'n_mels': n_mels,\n",
    "#           'hop_length': hop_length,\n",
    "#           'mel_scale': 'htk',\n",
    "#         }\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {}\n",
    "# i = 0\n",
    "# def data_preprocessing(data):\n",
    "#     mfccs = []\n",
    "#     labels = []\n",
    "#     for (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id) in data:\n",
    "#         mfcc = mfcc_transform(waveform).squeeze(0).transpose(0, 1).to(device)\n",
    "#         rand = random.randrange(0, len(mfcc) - 300)\n",
    "#         mfccs.append(mfcc[rand : rand + 300,:])\n",
    "#         if speaker_id in dic:\n",
    "#             labels.append(dic.get(speaker_id))\n",
    "#         else:\n",
    "#             global i\n",
    "#             dic[speaker_id] = i\n",
    "#             labels.append(i)\n",
    "#             i=i+1\n",
    "\n",
    "#     mfccs = torch.nn.utils.rnn.pad_sequence(mfccs, batch_first=True).transpose(1, 2)\n",
    "#     labels = torch.Tensor(labels)\n",
    "    \n",
    "#     return mfccs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_mfccs_251'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6cea6150c25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmfccs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_mfccs_251'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_labels_251'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmfccs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_mfccs_251'"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 64\n",
    "\n",
    "mfccs = torch.load('train_mfccs_251')\n",
    "labels = torch.load('train_labels_251')\n",
    "train_data = torch.utils.data.TensorDataset(mfccs, labels)\n",
    "mfccs = torch.load('test_mfccs_251')\n",
    "labels = torch.load('test_labels_251')\n",
    "test_data = torch.utils.data.TensorDataset(mfccs, labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size_train,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True\n",
    "                                           )\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=batch_size_test,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True\n",
    "                                         )\n",
    "print('train:', len(train_data), 'test:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():    \n",
    "    fig = plt.figure()\n",
    "    counter = [i for i in range(1, itera_test+1)]\n",
    "    plt.plot(counter, test_losses, color='red')\n",
    "    plt.plot(counter, avg_train_loss, color='black')\n",
    "    plt.legend(['Test Loss', 'Average Train Loss'], loc='upper right')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotacc():\n",
    "    fig = plt.figure()\n",
    "    acc_counter = [i for i in range(1, itera_test+1)]\n",
    "    plt.plot(acc_counter, acc, color='red')\n",
    "    plt.legend(['Accuracy'], loc='upper right')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xvecTDNN(nn.Module):\n",
    "\n",
    "    def __init__(self, numSpkrs, p_dropout, momentum):\n",
    "        super(xvecTDNN, self).__init__()\n",
    "        \n",
    "        self.tdnn1 = nn.Conv1d(in_channels=40, out_channels=512, kernel_size=5, dilation=1)\n",
    "        self.bn_tdnn1 = nn.BatchNorm1d(512, momentum=momentum, affine=False)\n",
    "        self.dropout_tdnn1 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.tdnn2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=5, dilation=2)\n",
    "        self.bn_tdnn2 = nn.BatchNorm1d(512, momentum=momentum, affine=False)\n",
    "        self.dropout_tdnn2 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.tdnn3 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=7, dilation=3)\n",
    "        self.bn_tdnn3 = nn.BatchNorm1d(512, momentum=momentum, affine=False)\n",
    "        self.dropout_tdnn3 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.tdnn4 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=1, dilation=1)\n",
    "        self.bn_tdnn4 = nn.BatchNorm1d(512, momentum=momentum, affine=False)\n",
    "        self.dropout_tdnn4 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.tdnn5 = nn.Conv1d(in_channels=512, out_channels=1500, kernel_size=1, dilation=1)\n",
    "        self.bn_tdnn5 = nn.BatchNorm1d(1500, momentum=momentum, affine=False)\n",
    "        self.dropout_tdnn5 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(3000, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512, momentum=momentum, affine=False)\n",
    "        self.dropout_fc1 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(512, momentum=momentum, affine=False)\n",
    "        self.dropout_fc2 = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, numSpkrs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_tdnn1(F.relu(self.bn_tdnn1(self.tdnn1(x))))\n",
    "        x = self.dropout_tdnn2(F.relu(self.bn_tdnn2(self.tdnn2(x))))\n",
    "        x = self.dropout_tdnn3(F.relu(self.bn_tdnn3(self.tdnn3(x))))\n",
    "        x = self.dropout_tdnn4(F.relu(self.bn_tdnn4(self.tdnn4(x))))\n",
    "        x = self.dropout_tdnn5(F.relu(self.bn_tdnn5(self.tdnn5(x))))\n",
    "        \n",
    "        stats = torch.cat((x.mean(dim=2), x.std(dim=2)), dim=1)\n",
    "        x = self.dropout_fc1(F.relu(self.bn_fc1(self.fc1(stats))))\n",
    "        x = self.dropout_fc2(F.relu(self.bn_fc2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "log_interval = int(2280 / batch_size_train)\n",
    "model = xvecTDNN(numSpkrs=251, p_dropout=0, momentum=0.1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "avg_train_loss = []\n",
    "avg_train_counter = []\n",
    "itera = 1\n",
    "def train():\n",
    "    global itera\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        mfccs, labels = data \n",
    "        mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mfccs)\n",
    "        loss = criterion(output, labels.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "#         train_losses.append(loss.item())\n",
    "#         train_counter.append((batch_idx*batch_size_train) + ((itera-1)*len(train_loader.dataset)))\n",
    "        # 每隔10个batch输出，batch_size=8,所以每隔80个数据输出一次\n",
    "        if batch_idx != 0 and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(mfccs), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                running_loss / log_interval))\n",
    "                # sum(train_losses) / batch_idx))\n",
    "                # loss.item()))\n",
    "            train_losses.append(running_loss / log_interval)\n",
    "            train_counter.append((batch_idx*batch_size_train) + ((itera-1)*len(train_loader.dataset)))\n",
    "            running_loss = 0\n",
    "    print('\\nTrain set: Avg. loss: {:.4f}\\n'.format(train_loss))\n",
    "    avg_train_loss.append(train_loss)\n",
    "    avg_train_counter.append(itera*len(train_loader.dataset))\n",
    "    scheduler.step()         \n",
    "    itera += 1\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_counter = []\n",
    "acc = []\n",
    "itera_test = 1\n",
    "\n",
    "def test():\n",
    "    global itera_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    class_in=[] #定义类内相似度列表\n",
    "    class_each=[] #定义类间相似度列表\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            mfccs, labels = data \n",
    "            mfccs, labels = mfccs.to(device), labels.to(device)\n",
    "            output = model(mfccs)\n",
    "            loss = criterion(output, labels.long())\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            # output.shape: [batchsize, 251]\n",
    "            \n",
    "            # get the index of the max log-probability in rows \n",
    "            # return the corresponding label\n",
    "            value, pred = torch.max(output.data, dim=1)\n",
    "\n",
    "            print(value)\n",
    "\n",
    "            # pred.shape && labels.shape: [batchsize]\n",
    "            # .eq逐个元素判断是否相等 相等1 不相等0\n",
    "            correct += pred.eq(labels).sum()\n",
    "            \n",
    "            # 需要识别的用户id和模型id一样，就认为是类内测试\n",
    "            # 否则是类间测试\n",
    "            # 对象是概率值\n",
    "            for i in range(batch_size_test):\n",
    "                if pred[i] == labels[i]:\n",
    "                    class_in.append(value[i].cpu().numpy())\n",
    "                else:\n",
    "                    class_each.append(value[i].cpu().numpy())\n",
    "    \n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset))) \n",
    "\n",
    "    test_losses.append(test_loss)\n",
    "    test_counter.append(len(train_loader.dataset)*itera_test)\n",
    "    acc.append(correct.detach().cpu().numpy() / len(test_loader.dataset))\n",
    "    \n",
    "    plot()\n",
    "    plotacc()\n",
    "    itera_test += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train()\n",
    "    test()\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print('Finished training and testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dic.items():\n",
    "    print('{key}:{value}'.format(key = key, value = value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
